# mnist-optimizer_compare
Deep learning class project to compare performance of different optimizers when training with MNIST data.  
Compared Adam, SGD, Adagrad, and RMSProp.

Comparison results:
![Results graph](https://github.com/jackomcw/mnist-optimizer_compare/blob/1bc188ecf4e3f81a46c1f849a18d45150dda50f0/optimizer-results.png)
